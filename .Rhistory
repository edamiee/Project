y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit <- lm(y~x)
dfbetas(fit)
library(datasets)
head(mtcars)
**
d1 <- mtcars
d1 <- d1[, names(d1) != "cyl"]
summary(lm(mpg ~ ., data = d1))$coefficients
d1 <- mtcars
d1 <- d1[, names(d1) != "vs"]
summary(lm(mpg ~ ., data = d1))$coefficients
d1 <- mtcars
d1 <- d1[, names(d1) != "carb"]
summary(lm(mpg ~ ., data = d1))$coefficients
library(swirl)
swirl()
ravenData
mdl<- glm(ravenWinNum~ravenScore)
mdl<- glm(ravenWinNum)
mdl<- glm()
mdl<- glm(ravenWinNum~ravenScore, data=ravenData)
mdl <- glm(ravenWinNum ~ ravenScore, binomial, ravenData)
predict(mdl, data.frame(ravenScore=c(0, 3, 6))
lodds<- predict(mdl, data.frame(ravenScore=c(0, 3, 6))
lodds<- predict(mdl, data.frame(ravenScore=c(0, 3, 6)
lodds<- predict(mdl, data.frame(ravenScore=c(0, 3, 6)))
exp(lodds)/(1+exp(lodds))
summary(mdl)
confit(mdl)
confint(mdl)
exp(confint(mdl))
anaova(mdl)
anova(mdl)
qchisq(0.95, 1)
var(rpois(1000, 50))
nxt()
View(hits)
class(hits[,'date'])
as.integer(head(hits[,'date']))
mdl<- glm(visits ~ date, poisson, hits)
summary(mdl)
confint(mdl,'date')
exp(confint(mdl, 'date'))
which.max(hits[,'visits'])
hits[704,]
mdl$fitted.values[704]
lambda <- mdl$fitted.values[704]
qpois(.95,lambda)
offset=log(visits+1)
mdl2 <- log(visits+1)
mdl2 <- qpois()
mdl2 <- qpois(offset)
mdl2 <- qpois(.5,lambda)
swirl()
mdl2
mdl2 <- glm(formula = simplystats ~ date, family = poisson, data = hits, offset = log(visits +1))
qpois(.95,mdl2$fitted.values[704])
library(MASS)
data(shuttle)
fit1 <- glm(use ~ wind - 1, data = shuttle, family = "binomial")
fit1
-0.2513/-0.2831
windhead <- fit1$coef[1]
windtail <- fit1$coef[2]
exp(windtail)/exp(windhead)
fit2 <- glm(use ~ wind + magn - 1, data = shuttle, family = "binomial")
windhead2 <- fit2$coef[1]
windtail2 <- fit2$coef[2]
exp(windtail2)/exp(windhead2)
shuttle$auto <- as.numeric(shuttle$use=="auto")
fit <- glm(auto ~ wind,  binomial,  shuttle)
fit3 <- glm(1-auto ~ wind,  binomial, shuttle)
fit$coefficients
fit3$coefficients
fit <- glm(count ~ spray  - 1, family = "poisson", data = InsectSprays)
?exp
exp(fit$coef[1])/exp(fit$coef[2])
data(mtcars)
fit1 <- lm(mpg ~ factor(cyl) + wt, data = mtcars)
fit1
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
lhs <- function(x) ifelse(x < 0,0-x,0) # basis function 1 (lhs = left hockey stick)
rhs <- function(x) ifelse(x > 0,x-0,0) # basis function 2 (rhs = right hockey stick)
gb <- lm(y ~ lhs(x) + rhs(x))
x <- seq(-5,5,by=1)
py <- gb$coef[1]+gb$coef[2]*lhs(x)+gb$coef[3]*rhs(x)
lines(x, py)
lhs <- function(x) ifelse(x < 0,0-x,0)
rhs <- function(x) ifelse(x > 0,x-0,0)
gb <- lm(y ~ lhs(x) + rhs(x))
x <- seq(-5,5,by=1)
py <- gb$coef[1]+gb$coef[2]*lhs(x)+gb$coef[3]*rhs(x)
lines(x, py)
py
gb
library.install("caret")
?install
install.packages("caret")
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
ggplot(data = training, aes(x = Superplasticizer)) + geom_histogram() + theme_bw()
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain, ]
testing = adData[-inTrain, ]
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
install.packages("Hmisc")
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
library("Hmisc")
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
library(Hmisc)
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
impute
library(AppliedPredictiveModeling)
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
install.packages("e1071")
library("e1071")
set.seed(3433)
## grep the predictors starting with 'IL'
IL_str <- grep("^IL", colnames(training), value = TRUE)
## make a subset of these predictors
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
## train the data using the first method
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
## get the confustion matrix for the first method
C1 <- confusionMatrix(predictions, testing$diagnosis)
print(C1)
install.packages("devtools")
library(devtools)
install_github('slidify', 'ramnathv')
install_github('slidifyLibraries', 'ramnathv')
library(slidify)
istall.packages("randomForest")
istall.package("randomForest")
install.packages("randomForest")
install.packages("rpart")
install.packages("rpart.plot")
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
rfNews()
pwd
pwd()
getwd()
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
install.packages(rattle)
set.seed(125)
install.package("rattle")
install.packages("rattle")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
install.packages(rattle)
set.seed(125)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
library(rpart)
install.packages(rattle)
set.seed(125)
inTrain <- createDataPartition(y = segmentationOriginal$Case, list = FALSE)
train <- subset(segmentationOriginal, Case == "Train")
test <- subset(segmentationOriginal, Case == "Test")
modFit <- train(Class ~ ., data = train, method = "rpart")
modFit$finalModel
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n = TRUE, all = TRUE, cex = .8)
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit)
predict(modFit, newdata = train)
library(rattle)
summary(segmentationOriginal$Case)
inTrain <- grep("Train",segmentationOriginal$Case)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
fit <- train(Class~.,data=training,method="rpart")
fancyRpartPlot(fit$finalModel)
predData <- training[1:3,]
which(colnames(training)=="TotalIntenCh2")
which(colnames(training)=="FiberWidthCh1")
which(colnames(training)=="PerimStatusCh1")
#TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2
#FiberWidthCh1 = 8;VarIntenCh4 = 100; PerimStatusCh1=2
predData[1,c(103,50,85)]=c(23000,10,2)
predData[2,c(103,50,85)]=c(50000,10,100)
predData[3,c(103,50,85)]=c(57000,8,100)
predict(fit,predData)
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
fit <- train(Area~.,data=olive,method="rpart")
pred <- predict(fit,newdata)
fancyRpartPlot(fit$finalModel)
install.packages("pgmm")
data(olive)
install.packages("olive")
data(olive)
library(pgmm)
data(olive)
library(pgmm)
data(olive)
olive = olive[,-1]
newdata = as.data.frame(t(colMeans(olive)))
fit <- train(Area~.,data=olive,method="rpart")
pred <- predict(fit,newdata)
fancyRpartPlot(fit$finalModel)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.package("ElemStatLearn")
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
model <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
data = trainSA, method = "glm", family = "binomial")
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd, predict(model, newdata = testSA))
missClass(trainSA$chd, predict(model, newdata = trainSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
libary(rpart)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
?randomForest
install.packages('rpart')
install.packages("rpart")
fit <- randomForest(y~.,data=vowel.train)
install.packages("randomForest")
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
library(randomForest)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
vowel = rbind(vowel.test,vowel.train)
vowel$y = factor(vowel$y)
vowel.train$y = factor(vowel.train$y)
set.seed(33833)
fit <- randomForest(y~.,data=vowel.train)
imps <- varImp(fit)
order(imps)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
a <- randomForest(y ~ ., data = vowel.train, importance = FALSE)
b <- varImp(a)
order(b)
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret package.
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
library(ElemStatLearn)
library(randomForest)
library(caret)
data(vowel.train)
data(vowel.test)
# Set the variable y to be a factor variable in both the training and test set.
# Then set the seed to 33833. Fit (1) a random forest predictor relating the
# factor variable y to the remaining variables and (2) a boosted predictor using
# the "gbm" method. Fit these both with the train() command in the caret package.
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Set the seed to 62433 and predict diagnosis with all the other variables using
# a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis
# ("lda") model. Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set? Is it better or worse than
# each of the individual predictions?
set.seed(62433)
# create models
fit1 <- train(diagnosis ~ ., data = training, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(diagnosis ~ ., data = training, method = "gbm")
library(shiny)
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
head(quakes)
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
head(Murder$USArrests)
head(USArrests$Murder)
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
head(WorldPhones)
head(USAssualts)
head(USArrests)
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
selected=names(iris)[[2]])
names(iris)[[2]])
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
head(USArrests)
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
install.packages('devtools')
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='edamiee', token='F14DC69CF9C518AD15246DCA0054F340', secret='1EI/ZgScCXSK0XckBu1MBK8EXMr6vB0CtewXig6F')
library(shinyapps)
shinyapps::deployApp('/Users/dame81/Desktop/DS_classes/Coursework/Data_Products/Project')
date()
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
shiny::runApp('Desktop/DS_classes/Coursework/Data_Products/Project')
getwd()
setwd("/Users/dame81/Desktop/DS_classes/Coursework/Data_Products/Project")
shinyapps::setAccountInfo(name='edamiee', token='F14DC69CF9C518AD15246DCA0054F340', secret='1EI/ZgScCXSK0XckBu1MBK8EXMr6vB0CtewXig6F')
deployApp()
library(pryr)
ftype(mean) # [1] "s3"  "generic"
# a generic function in a fresh installation of R,
# with only the default packages loaded
ftype(predict) # [1] "s3" "generic"
# a generic function in a fresh installation of R,
# with only the default packages loaded
ftype(show) # [1] "s4"  "generic"
ftype(lm) # [1] "function"
ftype(colSums) # [1] "internal"
ftype(dgamma) # [1] "function"
?dgamma
dgamma
library(ElemStatLearn)
library(caret)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
fitRf <- train(y ~ ., data=vowel.train, method="rf")
fitGBM <- train(y ~ ., data=vowel.train, method="gbm")
